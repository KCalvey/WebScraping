{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scraping Project\n",
        "Team Members: Joseph, Ryver, and Kari"
      ],
      "metadata": {
        "id": "qtJ-9WAP_itY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2LSnpBS50GT"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the URLs\n",
        "base_url = 'https://www.mathjobs.org/jobs/list'\n",
        "list_url = 'https://www.mathjobs.org/jobs?joblist-0-0------'"
      ],
      "metadata": {
        "id": "zWYrYMTsAD04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the job listing page\n",
        "response = requests.get(list_url)\n",
        "soup = bs(response.content, 'html.parser')"
      ],
      "metadata": {
        "id": "8S3qcTboAHa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get job listing containers\n",
        "job_div_tag_list = soup.find_all('div', class_='clr')"
      ],
      "metadata": {
        "id": "tZ_IAsr3EAt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store date\n",
        "data = {\n",
        "    'Institution Name': [],\n",
        "    'Position ID': [],\n",
        "    'Position Title': [],\n",
        "    'Position Location': [],\n",
        "    'Subject Area': [],\n",
        "    'Application Deadline': [],\n",
        "    'Apply Link': [],\n",
        "    'Job Details Link': []\n",
        "}"
      ],
      "metadata": {
        "id": "ztcqnxGREH3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for job_div in job_div_tag_list:\n",
        "    a_tag = job_div.find('a')\n",
        "if a_tag:\n",
        "        inst_name = a_tag.text.strip()\n",
        "        job_link = base_url + '/' + a_tag['href'].strip()\n",
        "\n",
        "        try:\n",
        "          job_response = requests.get(job_link)\n",
        "          job_soup = bs(job_response.content, 'html.parser')\n",
        "          details = job_soup.get_text(separator=' \\n')\n",
        "\n",
        "          def extract_between(text, start, end):\n",
        "            try:\n",
        "              return text.split(start)[1].split(end)[0]\n",
        "            except:\n",
        "              return ''\n",
        "\n",
        "          pos_id = extract_between(details, 'Job ID:', '\\n')\n",
        "          pos_title = extract_between(details, 'Position Title:', '\\n')\n",
        "          location = extract_between(details, 'Location:', '\\n')\n",
        "          subject = extract_between(details, 'Subject Area:', '\\n')\n",
        "          deadline = extract_between(details, 'Application Deadline:', '\\n')\n",
        "\n",
        "          apply_a = job_soup.find('a', text='Apply')\n",
        "          apply_link = base_url + apply_a['href'] if apply_a and 'href' in apply_a.attrs else ''\n",
        "\n",
        "        except Exception as ex:\n",
        "          print(f'Error processing {job_link} - {ex}')\n",
        "          pos_id = pos_title = location = subject = deadline = apply_link = ''\n",
        "\n",
        "        for key in data.keys():\n",
        "          if key == 'Institution Name':\n",
        "            data[key].append(inst_name)\n",
        "          elif key == 'Position ID':\n",
        "            data[key].append(pos_id)\n",
        "          elif key == 'Position Title':\n",
        "            data[key].append(pos_title)\n",
        "          elif key == 'Position Location':\n",
        "            data[key].append(location)\n",
        "          elif key == 'Subject Area':\n",
        "            data[key].append(subject)\n",
        "          elif key == 'Application Deadline':\n",
        "            data[key].append(deadline)\n",
        "          elif key == 'Apply Link':\n",
        "            data[key].append(apply_link)\n",
        "          elif key == 'Job Details Link':\n",
        "            data[key].append(job_link)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2fGR8OcHEbt",
        "outputId": "beaba70e-3181-432e-f50b-ea8e18ccfa01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-b6ddb75fb2cd>:24: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  apply_a = job_soup.find('a', text='Apply')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data)\n",
        "df.to_csv('mathjobs.csv', index=False)\n",
        "\n",
        "print(\"Scraping complete. Data saved to 'mathjobs.cvs'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAH3YqZnMplK",
        "outputId": "eb78bb54-658c-472b-fab6-dbcf65867118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping complete. Data saved to 'mathjobs.cvs'.\n"
          ]
        }
      ]
    }
  ]
}